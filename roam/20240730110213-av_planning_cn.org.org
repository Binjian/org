# Created 2024-07-30 Tue 11:08
#+options: toc:nil
#+options: num:nil
#+title: AV-Planning_CN
#+author: Binjian Xin
#+latex_compiler: xelatex
* 多模态(多目标行为和轨迹预测)
** 区分认识论与偶然论的多模态（epistemic vs aleatoric）：前者可通过更新测量消除，后者无法消除
** 表示应该是多模态的：比如非端到端采用混合高斯模型
** 端到端从数据中学习多模态
*** 潜在模型（CVAE）
*** 扩散模型（Diffusion)
** 保持多模态分布并从感知中实时更新，随时间演变成最终几个最终无法消除的多模态或单一模态
* 特定驾驶风格自适应匹配
** DAgger,简单，实用，有效
** RLHF（使用Bradley-Terry模型的奖励模型[比较]，在线奖励模型与在线HF，state（运动轨迹），action）
*** 使用回归loss,可切换至DPO方法，RL->常规MLE，稳定，高效训练
*** 具体奖励：行为策略与目标策略之间的KL散度
*** 抽象奖励：最小化观察分布漂移->修复数据分布漂移
** 基于RL的扩散策略
*** 使用ControlNet
* 扩散模型为何高效？
** 多模态概率分布拟合的SOTA：通过从数据分布中采样数据来近似任意复杂（包括多模态）分布
** 通过得分函数（Score Function)把原始随机分布分布迁移到数据样本分布(真实分布)
** 如果数据分布漂移，通过从最新数据中采样来适应学习模型到新的数据分布
*** 通过强化学习训练扩散模型
** 训练遵循与常规深度学习相同的方法
** 数据分布拟方法比较
*** GAN：由于博弈不稳定
*** VAE：多模态是超参数Z（潜在维度），需要调试
*** 流模型：效率低下
* 隐空间（内嵌）
** 用于端到端训练的内嵌层始终优于预训练的tokens
* 基于学习的方法（端到端）
** 生成/收集样本 --> 通过采样改进：采样/数据分布来近似真实统计数据
*** 期望
*** 梯度
*** 分布
** 用RL解决开放性问题
