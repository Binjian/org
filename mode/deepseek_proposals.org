#+title: DeepSeek调研报告及应用展望
#+AUTHOR: 忻斌健
#+CREATOR: 忻斌健
#+DATE:<2025-01-02 周四>
#+STARTUP: latexpreview
#+LATEX_COMPILER: xelatex
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper, 11pt]
#+LATEX_HEADER: \usepackage{svg}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{positioning,shapes.symbols, calc}
#+LATEX_HEADER: \usepackage{tikzmark}
#+LANGUAGE: zh-CN
#+OPTIONS: tex:t
#+OPTIONS: ^:{}
#+bind: org-export-publishing-directory "./exports"
#+DOWNLOAD_IMAGE_DIR:  '~/.org.d/mode/img'
#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:t reveal_control:t
#+OPTIONS: reveal_mathjax:t reveal_rolling_links:t reveal_keyboard:t reveal_overview:t num:nil
#+OPTIONS: reveal_width:1280 reveal_height:800
#+OPTIONS: toc:1
#+REVEAL_INIT_OPTIONS: transition: 'cube'
#+REVEAL_MARGIN: 0.005
#+REVEAL_MIN_SCALE: 0.01
#+REVEAL_MAX_SCALE: 2.5
#+REVEAL_THEME: sky
#+REVEAL_HLEVEL: 1
#+REVEAL_EXTRA_CSS: ./templates/drl101.css
#+REVEAL_PLUGINS: (highlight notes)
#+REVEAL_TITLE_SLIDE: ./templates/title_deepseek_proposals.html
#+REVEAL_TITLE_SLIDE_BACKGROUND: ./img/deepseek/ds_logo.png
#+REVEAL_TITLE_SLIDE_BACKGROUND_SIZE: 1600px
#+REVEAL_TITLE_SLIDE_BACKGROUND_OPACITY: 0.5
#+HTML_HEAD_EXTRA: <style> .figure p {text-align: center;}</style>
#+HTML_HEAD_EXTRA: <style>*{font-family: "LXGW WenKai Mono" !important}</style>
#+MACRO: color @@html:<font color="$1">$2</font>@@

* 背景
** 深度学习与神经网络
#+ATTR_REVEAL: :frag (appear)
- 基于机器学习
- 神经网络
  - 可从数据中学习，可以碎片化学习
  - 学习能力强
  - 学习容量大
- 强化学习：
  - 数据饥渴
  - 可以从复杂系统的碎片化经验中学习
** 苦涩的教训(Rich Sutton)
#+begin_quote
大部分人工智能和强化学习领域的进步来源于利用大量计算资源和通用学习算法，而不是依赖领域专家手工设计的特定知识。
#+end_quote
#+ATTR_REVEAL: :frag (appear)
- 学习算法的优势(规模化能力)
  #+ATTR_REVEAL: :frag (appear)
  - 专门设计的系统不利于规模化部署
  - 长期来看依赖计算和数据得来的策略更加稳健和高效
  - 通用算法能随着算力增加而不断提升表现
- 自动发现的重要性
  #+ATTR_REVEAL: :frag (appear)
  - 让系统通过数据和计算自动发现问题的最佳解
  - 非在细节上进行过多手工调优
  - 解除模型学习的限制
- 数据驱动
   #+begin_notes
   - 规模化能力、
     - 短期内利用人工经验可能有帮助，
     - 专家系统：需要工程团队维护规则算法，随着系统复杂度增加（必然性）不可维护
     - 比人类预先嵌入的智慧更为持久且具适应性
   - 自动发现有利于工程化
     - 将精力放在利用大规模计算和数据上
     - 推动了深度学习及强化学习等领域的革命性进步
   - 数据驱动：高质量数据非常重要
   #+end_notes
* 模型的演变
#+ATTR_REVEAL: :frag (appear)
- 开源最前沿模型(V0)
  - 训练(_SFT,DPO,Flash Attention_, bf16+fp32, _vLLM_, BBPE, _RoPE_, _MTP_, ZeRO)
  - 网络基本架构 _LLaMA_ + _RMSNorm_ + _SwiGLU_, _GQA_
- 提取高质量数据集(V0~R1)
  - 高质量数据集(2T), DeepSeekMath,CoT,代码
- *增量式创新* (V1~Zero)
  - 细颗粒力度混合专家架构* (MoE): 2+64/4+128/1+256
  - 多头隐注意力 (MLA)
  - 数据路由均衡 (端到端训练)
- 训练方法上的创新(R1)
  - 纯强化学习训练: *GRPO*

#+begin_notes
- 历史
  - LLM->MoE->V2->V3->Math->Zero->R1
  - 2024.01~2025.01
  - _GPT4时代还没有_
  - Mixtral 0/8 ➡GPT4➡DeepSeekMoE➡V3
  - 训练方法上的创新
    - 冷启动数据训练
    - 分阶段训练
    - 微调训练与后训练，附加强化学习训练
    - 蒸馏:基于QWen2.5/Llama3 (优于纯RL)，
#+end_notes

** 主要特点
#+ATTR_REVEAL: :frag (appear)
- 开源大模型(权重开放，方法开放，非常宽松的MIT许可)
  - 已经被多次复现
- 较强的推理能力
  - 来自数学知识和代码训练样本
  - 大模型的推理能力可蒸馏到小模型
- 高效(较低成本)
  - 架构：训练和推理稀疏化(MoE) + 内嵌瓶颈层(MLA) + (MTP）
  - 硬件驱动: 匹配通信约束跨节点数据流,低精度浮点数计算

#+begin_notes
  - 国内其他大模型公司:科大讯飞，腾讯云，百度，阿里千问,华为盘古：模型和应用？
  - 24年底，六小龙大模型公司的减法： 商汤日日新,零一万物,百川,智谱GLM,月之暗面Kimi,MiniMax海螺AI？
#+end_notes
** 启示
#+ATTR_REVEAL: :frag (appear)
- 简单架构
  - 通用人工智能
  - SoTA+递增式改进+实验验证
- 提高学习的效率,重点在数据收集和模型适配训练
  - 数据
  - 架构（MLA，编码容量瓶颈)
  - 通信（适配大数据动态）
- 推理能力可以蒸馏到较小模型（大模型的推理能力是关键）
* 应用
#+ATTR_REVEAL: :frag (appear)
- 制造与工业自动化: R1 模型可用于 自动化装配线 和 质量检测。
  - 精准装配：R1 可以帮助机器人准确地执行装配任务，减少错误和不合格品。
  - 质量控制：通过视觉系统和传感器数据，R1 能够实时检测产品缺陷，确保生产质量。
- 数据处理
  - OA助手
  - 编程
- 机器人
  - X1
  - 焊接机器人
  - 移动规划
