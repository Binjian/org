:PROPERTIES:
:ID:       a52aa49d-d9d0-4b3f-ba2b-d5eced50e7c6
:END:
#+title: 博弈,最优控制和机器学习
#+AUTHOR: 忻斌健
#+CREATOR: 忻斌健
#+DATE:<2025-01-02 周四>
#+STARTUP: latexpreview
#+LANGUAGE: zh-CN
#+OPTIONS: tex:t
#+OPTIONS: ^:{}
#+bind: org-export-publishing-directory "./exports"
#+DOWNLOAD_IMAGE_DIR:  '~/.org.d/mode/img'
#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:t reveal_control:t
#+OPTIONS: reveal_mathjax:t reveal_rolling_links:t reveal_keyboard:t reveal_overview:t num:nil
#+REVEAL_MATHJAX_URL: https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-svg-full.js
#+OPTIONS: reveal_width:1200 reveal_height:800
#+OPTIONS: toc:1
#+REVEAL_INIT_OPTIONS: transition: 'cube'
#+REVEAL_MARGIN: 0.005
#+REVEAL_MIN_SCALE: 0.01
#+REVEAL_MAX_SCALE: 2.5
#+REVEAL_THEME: sky
#+REVEAL_HLEVEL: 1
#+REVEAL_EXTRA_CSS: ./templates/drl101.css
#+REVEAL_PLUGINS: (highlight notes)
#+REVEAL_TITLE_SLIDE: ./templates/title_drl101.html
#+HTML_HEAD_EXTRA: <style> .figure p {text-align: center;}</style>
#+HTML_HEAD_EXTRA: <style>*{font-family: "LXGW WenKai Mono" !important}</style>
#+MACRO: color @@html:<font color="$1">$2</font>@@
#+MACRO: a @@html: <span class="fragment" data-fragment-index="$2">$1</span>@@
#+HTML_HEAD_EXTRA: <style type="text/css">.noboldheader th {font-weight: normal;}</style>
#+BEGIN_NOTES
  - Rip Van Winkel 华盛顿·欧文 1819，黄粱一梦，错过美国独立革命，纽约的民间故事
#+END_NOTES

* 目标

#+ATTR_REVEAL: :frag (appear)
- 最优控制和机器学习的共同基础
- 决策问题程序化
- 通过学习的方式获得最优决策

#+BEGIN_NOTES
- 理解强化学习，不需要理解深度学习
- 可以从强化学习的角度理解深度学习的本质
- 强化学习的背景，动机，强化学习本身只占较小的一部分
- 暗物质
  - 指基本事实经常会被忽略，或下意识假定，经常是错误的假定，而对理解问题是非常必要的前提。
- 机器人抓取任务里为何会使用随机决策?违反直觉!
  - 逻辑与直觉结合处理复杂现象
- 可以用基本的问题引出
  - 为什么是树结构？
  - 什么是机器学习？
#+END_NOTES
* 一个简单的决策问题
** 1.1 对决

#+begin_quote
游戏(博弈)
#+ATTR_REVEAL: :frag (appear)
- 游戏的要素:
  #+ATTR_REVEAL: :frag (appear)
  - 玩家
  - 收益(代价,奖励)
  - 策略
- 游戏的趣味：复杂，难度，平衡

#+end_quote

#+BEGIN_NOTES
  👉 王者荣耀
#+END_NOTES
*** 对决
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in cube-out'
:END:

@@html:<div class="r-stack">@@
        @@html:<img class="fragment fade-out" data-fragment-index="0" src="img/drl101/dominated_fight.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="0" src="img/drl101/dominated_fight0.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="1" src="img/drl101/dominated_fight1.png" />@@
        @@html:<img class="fragment" data-fragment-index="2" src="img/drl101/dominated_fight2.png" />@@
@@html:</div>@@
@@html:<span class="fragment"; style="color:darkgreen; font-weight:bold"; data-fragment-index="2">@@优势策略@@html:</span>@@

** 1.2 势均力敌
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in cube-out'
:END:

@@html:<div class="r-stack">@@
        @@html:<img class="fragment fade-out data-fragment-index="0" src="img/drl101/ne.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="0" src="img/drl101/ne1.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="1" src="img/drl101/ne2.png" />@@
        @@html:<img class="fragment" data-fragment-index="2" src="img/drl101/ne3.png" />@@
        @@html:<img class="fragment" data-fragment-index="4" src="img/drl101/mixed.png" style="height:400px" />@@
@@html:</div>@@
@@html:<span class="fragment"; style="color:darkgreen; font-weight:bold"; data-fragment-index="2">@@策略均衡状态@@html:</span>@@
@@html:<div class="fragment"; style="color:darkred; font-weight:bold"; data-fragment-index="3">@@?@@html:</div>@@

  #+BEGIN_NOTES
  - 没有优势策略
    - 当有些问题没有答案的时候，从另一个角度或层次会发现更有趣的现象或更重要问题
  - 行与列(两个对手)的策略重合，是更重要的问题
    - 一方选择攻击，另一方退出的状态下，没有任何一方愿意偏离当前的状态，平衡状态。（纳什均衡）
  #+END_NOTES

  #+BEGIN_NOTES
   - 前提条件是同时决策,不知道对方的策略!
   - 策略均衡限于稳定的平衡状态
   - 策略均衡是对双方最合理的最优状态：任何一方偏离均衡状态，而另一方保持理性决策，都会导致偏离方收益受损，所以没有任何一方愿意偏离均衡状态下的最优决策
   - A 非理性决策 vs B 理性决策
   - 理性决策优于非理性决策
   - 有几个均衡状态？
  #+END_NOTES

** 1.3 混合策略
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in none-out'
:END:
#+NAME: 混合策略
#+ATTR_HTML: :alt  :title 混合策略 :width 300px  :align center
#+attr_org: :width 300px :align left
[[./img/drl101/mixed1.png]]

#+ATTR_REVEAL: :frag (appear)
- 对手攻击收益：$\color{red}{PO^{f}=(-1)\times p^{A} + (2)\times (1-p^{A})}$
- 对手撤退收益：$\color{blue}{PO^{q}=(0)\times p^{A} + (0)\times (1-p^{A})}$
- $p^{A}=0.5$ ?
  #+ATTR_REVEAL: :frag (appear)
  - 我方收益 -0.5:1
  - 对手收益：0.5:0
- $p^A$ 何时最优?
  #+ATTR_REVEAL: :frag (appear)
  👉 让对方失去选择, 对$\forall\hspace{0.5em}p^{B}$
  #+BEGIN_NOTES
   - 我方的收益取决于对手的决策！
   - 对手的任何策略,收益都一样
  #+END_NOTES

*** 混合策略
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none'
:END:

#+ATTR_HTML: :alt  :title 混合策略 :width 300px  :align center
#+attr_org: :width 300px :align left
[[./img/drl101/mixed1.png]]

#+ATTR_REVEAL: :frag (appear)
- 我方策略:$\color{red}{PO^{f}}=\color{blue}{PO^{q}}$ 👉 $p^{A}=\frac{2}{1+2}=\frac{2}{3}$
  #+ATTR_REVEAL: :frag (appear)
  - 收益？
  - 均衡策略: 我方收益 $-\frac{2}{3}\times p^{B} + \frac{4}{3}\times (1-p^{B})$
  - $p^{A}=1$?
- 理性决策优于非理性决策
  #+BEGIN_NOTES
   - A 非理性决策 (p=1,0.5) vs B 理性决策 (p=2/3)
  #+END_NOTES
- 混合策略的均衡是对双方最合理的最优状态
  #+BEGIN_NOTES
   - 混合策略的均衡：任何一方偏离均衡状态，而另一方保持理性决策，都会导致偏离方收益受损，所以没有任何一方愿意偏离均衡状态下的最优决策
  #+END_NOTES
- 随机策略优于确定策略
  #+BEGIN_NOTES
   -随机是应对复杂现象的高效模型
   - 如何从随机策略中选择一个最优的策略? 对信号的概率分布进行运算,找出符合目标的最优策略.
  #+END_NOTES

*** 混合策略
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none-in cube-out'
:END:

#+ATTR_HTML: :alt  :title 混合策略 width 300px  :align center
#+attr_org: :width 400px :align left
[[./img/drl101/mixed.png]]

- 多轮持续对决？


** 1.4 在时间的长河里

*** 决策树
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in none-out'
:END:
#+REVEAL_HTML: <div class="gridded_frame_with_columns">
     #+REVEAL_HTML: <div class="one_of_2_columns">
        @@html:<div class="r-stack">@@
         @@html:<img class="fragment fade-out data-fragment-index="0" src="img/drl101/mixed1.png" />@@
         @@html:<img class="fragment current-visible" data-fragment-index="0" src="img/drl101/flat_tree.png" />@@
         @@html:<img class="fragment" data-fragment-index="1" src="img/drl101/flat_tree2.png" />@@
        @@html:</div>@@
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_2_columns">
        @@html:<div class="r-stack">@@
         @@html:<img class="fragment fade-out data-fragment-index="2" src="img/drl101/tree.png" />@@
         @@html:<img class="fragment" data-fragment-index="2" src="img/drl101/flat_tree3.png" />@@
        @@html:</div>@@
     #+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

#+BEGIN_NOTES
- 决策树对决策理论（强化学习）, 几乎是唯一的模型
- 对理解时间序列至关重要
#+END_NOTES

** 1.5 持续对决
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none-in cube-out'
:END:


@@html:<div class="r-stack">@@
        @@html:<img class="fragment fade-out data-fragment-index="0" src="img/drl101/tree21.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="0" src="img/drl101/tree3.png" />@@
        @@html:<img class="fragment" data-fragment-index="1" src="img/drl101/tree4.png" />@@
@@html:</div>@@

#+ATTR_REVEAL: :frag (appear)
- 逐级倒推: 从最后一轮开始分析
- 攻击发生概率 $\mathcal{P}=\frac{v}{v+c}: \frac{2}{3}\searrow 0, \textrm{if}\quad v: 2\searrow 0$
- 价值函数：当前决策和状态的长期价值

#+BEGIN_NOTES
- 时间序列的复杂度指数数增加！
- 生物演化也是树状
- 因果序列的分歧演化
#+END_NOTES

#+BEGIN_NOTES
- 最优决策需要考虑短期作用的长期后果
  - 存在一个基本规律(类似物理学的基本定律)，可以很容易地评估短期作用的长期后果
    - 自然界，经验积累
    - 最优控制，强化学习
- 如何评估这个后果？
  - 经验积累相似的方式，积累“价值函数”
#+END_NOTES


* 最优控制，强化学习与机器人
** 2.1 回顾
*** 十年前

#+REVEAL_HTML: <div class="gridded_frame_with_columns">
     #+REVEAL_HTML: <div class="one_of_2_columns">
        #+attr_org: :width 300px :align left
        #+REVEAL_HTML: <iframe title="PR2" width="600" height="450" src="https://www.youtube.com/embed/gYqfa-YtvW4" frameborder="0" allow="fullscreen; autoplay" allowfullscreen muted></iframe>
        #+REVEAL_HTML: <figcaption>PR2</figcatption>
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_2_columns">
        #+attr_org: :width 300px :align left
        #+REVEAL_HTML: <iframe title="ASIMO" width="600" height="450" src="https://www.youtube.com/embed/xjXUyLAHR1E" frameborder="0" allow="fullscreen; autoplay" allowfullscreen muted></iframe>
        #+REVEAL_HTML: <figcaption>ASIMO</figcatption>
     #+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

#+BEGIN_NOTES
- PR2
  - 2010年, willow garage (ROS吴恩达)
  - 执行器，传感器(深度相机,激光雷达)，本体,关节
  - 成本下降
  - 本体更仿生(更复杂)
- Asimo
  - 步态控制和现在机器人的区别
  - 现在: https://www.youtube.com/watch?v=6CjxMPg0pvg
#+END_NOTES

*** 最优控制

#+REVEAL_HTML: <iframe width="1024" height="576" src="https://www.youtube.com/embed/OmpzeWym7HQ#t=12m45s" frameborder="0" allow="fullscreen; autoplay" allowfullscreen muted></iframe>
#+REVEAL_HTML: <figcaption>John Tsitsiklis(OG)</figcatption>
#+BEGIN_NOTES
2019 talk
- 控制理论早期：PID 反馈控制， 线性控制,前提是线性系统，简洁优美
  - 无差别地运用到其他控制对象，导致复杂专家系统，复杂工程和系统,
  - 反馈控制理论:零极点补偿➡取消系统原有的动态,没有利用原有的系统动态
- 1990s,12:45, 最优控制（近似动态规划）与强化学习
  - 主要区别在于强化学习强调与环境互动，基于学习
  - 最优控制（近似动态规划, 自适应控制，鲁棒控制）强调系统辨识,模型近似
  - 浅层神经网络,没有深度学习
  - Dimitri P. Bertsekas
- 14m18s~15m57s;
- 将会简单评论AlphaGO算法
#+END_NOTES

*** 强化学习
#+REVEAL_HTML: <div class="gridded_frame_with_columns">
     #+REVEAL_HTML: <div class="one_of_2_columns">
        #+ATTR_HTML: :alt  :title Year_Of_RL :width 400px  :align center
        #+attr_org: :width 300px :align left
        [[./img/drl101/jim_fan.png]]
        #+REVEAL_HTML: <figcaption>2025强化学习之年</figcatption>
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_2_columns">
        #+ATTR_HTML: :alt  :title R1 :width 400px  :align center
        #+attr_org: :width 300px :align left
        [[./img/drl101/deepseek_r1_arxiv.png]]
        #+REVEAL_HTML: <figcaption>DeepSeek R1</figcatption>
     #+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

** 2.2 模型
#+ATTR_HTML: :border 2 :class noboldheader
| 博弈                                                 | @@html:玩家@@                                              | 收益@@html:<br>@@(代价)                          | 策略                                               | 状态                                             | 策略评估                                                         |
|------------------------------------------------------+------------------------------------------------------------+--------------------------------------------------+----------------------------------------------------+--------------------------------------------------+------------------------------------------------------------------|
| {{{a(<font color=darkblue>强化学习</font>,1)}}} | {{{a(<font color=darkblue>智能体/<br>系统</font>,1)}}} | {{{a(<font color=darkblue>奖励</font>,1)}}} | {{{a(<font color=darkblue>作用</font>,1)}}}   | {{{a(<font color=darkblue>状态</font>,1)}}} | {{{a(<font color=darkblue>价值函数</font>,1)}}}          |
| {{{a(<font color=red>最优控制</font>,2)}}}           | {{{a(<font color=red>控制器/<br>对象</font>,2)}}} | {{{a(<font color=red>误差</font>,2)}}} | {{{a(<font color=red>控制量</font>,2)}}} | {{{a(<font color=red>状态</font>,2)}}} | {{{a(<font color=red>目标函数</font>,2)}}}                     |

#+BEGIN_NOTES
  - 玩家：人类, vs 人类,计算机,自然/物理规律;
  - 计算机 vs. 自然/物理规律
  - 增加观测量和价值估计
    - 强化学习的价值函数是通过学习得到，是系统状态和作用的函数，考虑系统方程和系统动态
    - 最优控制的目标函数是专家规则，没有考虑系统动态!
#+END_NOTES

#+ATTR_REVEAL: :frag (appear)
#+attr_html: :alt :title 强化学习模型 :width 750pix :align center
#+NAME: 强化学习模型
#+attr_org: :width 300px :align left
[[./img/drl101/rl_model.png]]


** 2.3 强化学习的方法
#+ATTR_REVEAL: :frag (appear)
#+begin_quote
分步骤解决复杂问题
#+end_quote
  #+ATTR_REVEAL: :frag (appear)
  - “如给定现在，未来与过去无关”
    #+ATTR_REVEAL: :frag (appear)
    👉 马尔可夫决策过程
  - 复杂问题可分解为子问题
    #+ATTR_REVEAL: :frag (appear)
    👉 动态规划
  - 从碎片化的经验中估计状态和行动价值
    #+ATTR_REVEAL: :frag (appear)
    👉 贝尔曼方程
#+BEGIN_NOTES
  - 理解概念比记住概念的名称更重要
  - 动态规划是主流的经典概念,也是最优控制的基础
  - 从a走到b的最短路径,可分两个阶段a到c,c到b:假设c到b最短,那么只要解决a到c最短这一个子问题!
#+END_NOTES
*** 理性决策
#+ATTR_REVEAL: :frag (appear)
#+begin_quote
- 算法是理性决策
- 理性决策针对非理性决策是优势策略
#+end_quote

#+BEGIN_NOTES
  - AlphaGo 很难战胜，人很难战胜机器：完美记忆，纯粹理性，高效执行，可复制
  - 没有目的
  - Jeff Hinton的警告
  - 强化学习本来是人工智能领域里一个比较冷僻的方向,和最优控制最大的差异在于学习的概念.
    - 为何自2016年以来越来成为人工智能，机器人的主流？--> 深度学习。
    - 两者如何结合？采样！从碎片化经验中学习。
#+END_NOTES

** 2.4 从碎片化经验中学习

*** 随机采样
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in cube-out'
:END:

@@html:<div class="r-stack">@@
        @@html:<img class="fragment fade-out data-fragment-index="0" src="img/drl101/tree_sample0.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="0" src="img/drl101/tree_sample.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="1" src="img/drl101/tree_sample1.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="2" src="img/drl101/tree_sample2.png" />@@
        @@html:<img class="fragment" data-fragment-index="3" src="img/drl101/tree_sample3.png" />@@
@@html:</div>@@

*** 随机采样的好处

#+attr_html: :alt :title 随机决策采样 :width 500pix :align center
#+NAME: 随机决策采样
#+attr_org: :width 300px
[[./img/drl101/tree_sample1.png]]

#+ATTR_REVEAL: :frag (appear)
- 真实的数据
  #+ATTR_REVEAL: :frag (appear)
  - 建模的复杂度过高
- 复杂函数/分布：
  #+ATTR_REVEAL: :frag (appear)
  - 非线性
  - 时变过程与非平稳过程
- 自然规律
- 处理复杂问题的高效方式
- 可以从碎片化的经验中学习
#+BEGIN_NOTES
- 掷色子通常是复杂随机环境最高效的学习方式
- 数学上的确定性问题用概率方法去求解往往有简洁高效的的方式，（组合数学）
#+END_NOTES
*** 最优控制 vs. 强化学习

#+REVEAL_HTML: <div class="gridded_frame_with_columns">
     #+REVEAL_HTML: <div class="one_of_2_columns">
     @@html:<div class="r-stack">@@
        @@html:<img class="fragment" data-fragment-index="0" src="img/drl101/hiking.jpg" height="400px"/>@@
     @@html:</div>@@
     @@html:<div class="r-stack">@@
        @@html:<div class="centered"><span class="fragment" data-fragment-index="0">@@最优控制@@html:</span></div>@@
     @@html:</div>@@
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_2_columns">
     @@html:<div class="r-stack">@@
        @@html:<img class="fragment current-visible" data-fragment-index="1" src="img/drl101/surfing.jpg" height="400px"/>@@
        @@html:<img class="fragment" data-fragment-index="2" src="img/drl101/skateboarding.jpg" height="400px"/>@@
     @@html:</div>@@
     @@html:<div class="r-stack">@@
        @@html:<div class="centered"><span class="fragment current-visible" data-fragment-index="1">@@强化学习@@html:</span></div>@@
        @@html:<div class="centered"><span class="fragment" data-fragment-index="2">@@机器人强化学习@@html:</span></div>@@
     @@html:</div>@@
     #+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

** 2.5 模型复杂度

*** AlphaGo的状态和决策树
#+attr_html: :alt :title AlphaGo决策树 :width 800pix :align center
#+NAME: AlphaGo决策树
#+attr_org: :width 300px
[[./img/drl101/MCTS-in-AlphaGo.png]]

#+ATTR_REVEAL: :frag (appear)
- 价值: 可理解为胜率
*** AlphaGo的状态和决策树
#+attr_html: :alt :title AlphaGo决策树 :width 800pix :align center
#+NAME: AlphaGo决策树
#+attr_org: :width 300px
[[./img/drl101/alphago_mcts.png]]

*** AlphaGo的复杂度

#+ATTR_REVEAL: :frag (appear)
- 所有的位置（观测量） $3^{{19}^2}\approx 1.74\times 10^{172}$, $1.20\%$ 合法
- 平均~200步/局，不同棋局的平均数量 $~3\times 10^{511}$
- 理论最长步数 $10^{48}$, 不同棋局的数量:$[10^{10^{48}},10^{10^{171}}]$
- 可观测宇宙的原子个数 $10^{80}$
  #+ATTR_REVEAL: :frag (appear)
  👉  神经网络
#+BEGIN_NOTES
 - 原子个数: 爱丁顿数
 - 从完整的部份经验中学习: 从部份棋局中学习,累积学习经验
 - 从不完整的部份经验中学习: 在线学习,不等棋局结束,边干边学
 - 已经解决，令人惊叹！
   - 人类智慧和经验的总结：攻防，布局，死活，官子，联络，形势，手筋，攻防
   - 试图通过特征方法总结人类经验，完全不敌AlphaGo
 - 人工智能的苦涩教训
#+END_NOTES

*** 双足机器人的状态和复杂度

#+REVEAL_HTML: <div class="gridded_frame_with_columns">
     #+REVEAL_HTML: <div class="one_of_2_columns">
        #+REVEAL_HTML: <iframe width="600" height="450" src="https://www.youtube.com/embed/0OUavEtbt2E#t=6m03s" frameborder="0" allow="fullscreen; autoplay" allowfullscreen muted></iframe>
        #+REVEAL_HTML: <figcaption>Cassie模型</figcatption>
        #+BEGIN_NOTES
          - 再看一个具身智能的例子：cassie，双足机器人
          - 5m57s~6m27s, 7m08s~8:45s
            - 惯性/质量矩阵正定矩阵,高复杂性
            - 系统状态与动态,策略(控制器);
            - 目标(收益,控制轨迹),策略评估,(玩家)
          - 复杂性
            - 动力学系统, 控制量的长效影响
            - 部份可观测性/随机性
            - 非线性
            - 足式机器人:欠驱动系统,
              - 有意为之,控制有难度,但是更自然,更节能,自然的步态是最优的控制方案:用尽量少的能量,经济的方式进行运动控制.(控制量影响状态量的方式!)
        #+END_NOTES
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_2_columns">
        #+ATTR_REVEAL: :frag (appear)
        - 复杂对象的控制方式:
          #+ATTR_REVEAL: :frag (appear)
          - 最优控制
            #+BEGIN_NOTES
            - 拉格朗日力学，力/力矩➡行动/作用action:能量（动能与势能)变化的时间积分;力/能量变化产生运动;
            - 稳态作用原理(运动遵循能量均衡状态，守恒): 任何系统动态有唯一的路径
            - 自从1990s以来, 两种方法
            - 处理复杂现象没有简单有效的魔术方法，必须要消耗计算资源，关键是如何运用：或者用于系统辨识，或者用于分步分片消化经验数据。
            - 最优控制,近似动态规划 Approximate DP：精确的环境和动力学模型,抓住主要矛盾,缺点是模型的特异性,针对特殊场景和功能(难以泛化),抗干扰能力差(不健壮)
            #+END_NOTES
          #+ATTR_REVEAL: :frag (appear)
          - 强化学习
            #+BEGIN_NOTES
            - 随机和概率模型,通过学习的方式(自然和人处理和解决问题的方式)
            - 系统状态,策略通过学习得到
            - 强化学习为何能处理复杂问题?
            #+END_NOTES
        - 如何学习?
          #+BEGIN_NOTES
          - 主要是深度学习的突破
          - 实现里从碎片化经验中学习复杂的系统动态
          - 评估复杂价值函数,复杂的策略!
          #+END_NOTES
     #+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

** 2.6 机器人的机器学习
#+ATTR_REVEAL: :frag (appear)
- 每次演示是决策树上的一条路径
- 随机采样的数据密度
- 成功或失败的经验
#+BEGIN_NOTES
  - 强化学习训练
  - 覆盖特定功能的观测数据分布
  - 成功或失败的路径
#+END_NOTES

*** 仿真的作用
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in cube-cout'
:END:

#+REVEAL_HTML: <div class="gridded_frame_with_columns">
     #+REVEAL_HTML: <div class="one_of_3_columns">
        #+ATTR_HTML: :alt  :title  :width 400pix  :align center
        #+attr_org: :width 400px :align left
        #+CAPTION: 抓取
        #+NAME: pick
        [[https://developer-blogs.nvidia.com/wp-content/uploads/2022/07/image16.gif]]
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_3_columns">
        #+ATTR_HTML: :alt  :title tree :width 400pix  :align center
        #+attr_org: :width 400px :align left
        #+CAPTION: 定位
        #+NAME: position
        [[https://developer-blogs.nvidia.com/wp-content/uploads/2022/07/image5-1.gif]]
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_3_columns">
        #+ATTR_HTML: :alt  :title tree :width 400pix  :align center
        #+attr_org: :width 400px :align left
        #+CAPTION: 操作
        #+NAME: operation
        [[https://developer-blogs.nvidia.com/wp-content/uploads/2022/07/image6.gif]]
     #+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

*** 用于训练的仿真数据
#+CAPTION[robot leanring]: training dataset generation
#+REVEAL_HTML: <iframe width="640" height="360" src="https://www.youtube.com/embed/OAZrBYCLnaA" frameborder="0" allow="fullscreen; autoplay" allowfullscreen muted></iframe>
#+REVEAL_HTML: <figcaption>NVidia Isaac Sim</figcatption>
#+BEGIN_NOTES
  - 15:24 ~ 16:57
  - ACRONYM Nvidia FLEX
  - 想象一下用模型方式来描述
  - 应用工程师的技能要求：可能不需要编程
#+END_NOTES


*** 机器人学习的算法

#+ATTR_REVEAL: :frag (appear)
- 数据
  #+ATTR_REVEAL: :frag (appear)
  - 来源:在线/离线/(仿真)
  - 预训练(基础模型GPT)
  - 数据范式(训练规划/数据/多样性构造)
- 学习模型
  #+ATTR_REVEAL: :frag (appear)
  - 鲁棒性
  - 多样性
#+BEGIN_NOTES
  - 学习模型:代表学习,神经网络
  - 数据非常重要
    - 在线/离线
    - 预训练(基础模型GPT)
      -基础模型(常识和基础推理能力)，跨域学习（自动驾驶经验有助于人形机器人的性能）
    - 数据多样化非常重要,多形态的机器人数据更有意义:可训练同一个模型,平均性能改善50%以上
  - 高效学习模型,能学习复杂行为模式(多模态)
#+END_NOTES


** 2.7 理解AlphaGo

*** AlphaGo系统结构
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in none-out'
:END:

#+attr_html: :alt :title AlphaGo神经网络 :width 800pix :align center
#+NAME: AlphaGo神经网络
#+attr_org: :width 300px
[[./img/drl101/alphago_nn.png]]

#+ATTR_REVEAL: :frag (appear)
- 碎片化经验学习
  #+ATTR_REVEAL: :frag (appear)
  - 部分经验累积 👉 神经网络
  - 不完整经验累积 👉 在线学习
- 随机和概率是应对复杂现象的有效模型
  #+ATTR_REVEAL: :frag (appear)
  - 价值网络: 可理解为简单的胜率查找表
#+BEGIN_NOTES
- 可以下完一局学一局
- 可以边下边学(时序差分学习)
- 决策网络，
- 围棋复杂度极高,但是确定性游戏
#+END_NOTES

*** 最优策略
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none-in cube-out'
:END:
#+attr_html: :alt :title AlphaGo神经网络 :width 800pix :align center
#+NAME: AlphaGo神经网络
#+attr_org: :width 300px
[[./img/drl101/alphago_nn.png]]

#+ATTR_REVEAL: :frag (appear)
- 均衡策略
  #+ATTR_REVEAL: :frag (appear)
  - 混合策略的均衡是对双方最合理的最优状态
  - 理性决策优于非理性决策
- 自我训练/自我学习
  #+ATTR_REVEAL: :frag (appear)
  - 不断提升水平
#+ATTR_REVEAL: :frag (appear)
#+begin_quote
→ 均衡状态(最优策略)
#+end_quote

#+BEGIN_NOTES
 - 均衡策略为何是最优的策略?
   - 混合策略的均衡：任何一方偏离均衡状态，而另一方保持理性决策，都会导致偏离方收益受损，所以没有任何一方愿意偏离均衡状态下的最优决策
   - 直觉：先立于不败之地(防御)，才能战胜对手
 - A 非理性决策(人类棋手) vs B 理性决策(AlphaGo)
 - 纳什均衡: 自我博弈，我的决策必须让对手的收益在任何决策下是一样的
   - 自我训练/自我学习:左右互搏
   - 自我训练为何能提升水平
     - 数学上:在合理假设下（收益大于代价$v>c$,理性决策：平衡状态对应收益的一阶导数，平衡状态的二阶导数<0
#+END_NOTES

* 总结
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in none-out'
:END:

#+ATTR_REVEAL: :frag (fade-in) :frag_idx (1 2 3)
- 最优策略
  - 最优决策必须要考虑对手的决策
- 机器学习
  - @@html:<span class="r-stack">@@
    @@html:<span class="fragment fade-out"; data-fragment-index="4">@@随机采样是应对复杂问题的高效方法@@html:</span>@@
    @@html:<span class="fragment fade-in"; style="color:#FF0000; font-weight:bold"; data-fragment-index="4">@@随机采样是应对复杂问题的高效方法@@html:</span>@@
    @@html:</span>@@
- 神经网络
  - 随机和概率是应对复杂现象的有效模型
