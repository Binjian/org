:PROPERTIES:
:ID:       a52aa49d-d9d0-4b3f-ba2b-d5eced50e7c6
:END:
#+title: Game Theory, Optimal Control, and Machine Learning
#+AUTHOR: Xin Binjian
#+CREATOR: Xin Binjian
#+DATE:<2025-01-02 Thu>
#+STARTUP: latexpreview
#+OPTIONS: tex:t
#+OPTIONS: ^:{}
#+bind: org-export-publishing-directory "./exports"
#+DOWNLOAD_IMAGE_DIR:  '~/.org.d/mode/img'
#+OPTIONS: reveal_center:t reveal_progress:t reveal_history:t reveal_control:t
#+OPTIONS: reveal_mathjax:t reveal_rolling_links:t reveal_keyboard:t reveal_overview:t num:nil
#+REVEAL_MATHJAX_URL: https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-svg-full.js
#+OPTIONS: reveal_width:1200 reveal_height:800
#+OPTIONS: toc:1
#+REVEAL_INIT_OPTIONS: transition: 'cube'
#+REVEAL_MARGIN: 0.005
#+REVEAL_MIN_SCALE: 0.01
#+REVEAL_MAX_SCALE: 2.5
#+REVEAL_THEME: sky
#+REVEAL_HLEVEL: 1
#+REVEAL_EXTRA_CSS: ./templates/drl101.css
#+REVEAL_PLUGINS: (highlight notes)
#+REVEAL_TITLE_SLIDE: ./templates/title_drl101_en.html
#+HTML_HEAD_EXTRA: <style> .figure p {text-align: center;}</style>
#+HTML_HEAD_EXTRA: <style>*{font-family: "LXGW WenKai Mono" !important}</style>
#+macro: color @@html:<font color="$1">$2</font>@@
#+MACRO: a @@html: <span class="fragment" data-fragment-index="$2">$1</span>@@
#+BEGIN_NOTES
  - Rip Van Winkle, Washington Irving, 1819. Like a fleeting dream, missing the American Revolutionary War, a folk story from New York.
#+END_NOTES

* Objectives

#+ATTR_REVEAL: :frag (appear)
- Common foundation of optimal control and machine learning
- Programmatic approach to decision-making
- Attaining optimal decisions through learning

#+BEGIN_NOTES
- Understanding reinforcement learning does not require understanding deep learning.
- You can grasp the essence of deep learning from a reinforcement learning perspective.
- The background and motivation of reinforcement learning itself occupy only a small portion.
- Dark matter:
  - Refers to basic facts often overlooked or subconsciously assumedâ€”frequently incorrect assumptionsâ€”yet crucial for understanding issues.
- Why does a robot grasping task use random decision-making? Itâ€™s counterintuitive!
  - Combining logic and intuition to handle complex phenomena.
- Fundamental questions can lead into the topic:
  - Why use a tree structure?
  - What is machine learning?
#+END_NOTES

* A Simple Decision Problem
** 1.1 Duel

#+begin_quote
Game (Conflict)
#+ATTR_REVEAL: :frag (appear)
- Elements of a game:
  #+ATTR_REVEAL: :frag (appear)
  - Players
  - Payoff (Cost, Reward)
  - Strategy
- The fun of the game: complexity, difficulty, balance

#+end_quote

#+BEGIN_NOTES
  ğŸ‘‰ Honor of Kings
#+END_NOTES

*** Duel
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in cube-out'
:END:

@@html:<div class="r-stack">@@
        @@html:<img class="fragment fade-out" data-fragment-index="0" src="img/drl101/dominated_fight_en.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="0" src="img/drl101/dominated_fight0_en.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="1" src="img/drl101/dominated_fight1_en.png" />@@
        @@html:<img class="fragment" data-fragment-index="2" src="img/drl101/dominated_fight2_en.png" />@@
@@html:</div>@@
@@html:<span class="fragment"; style="color:darkgreen; font-weight:bold"; data-fragment-index="2">@@Dominant Strategy@@html:</span>@@

#+BEGIN_COMMENT

@@html:<div style="text-align: center;">@@
@@html:<span class+"fragment fade-in"; data-fragment-index="3">@@
@@html:<span style="color:darkgreen;font-weight:bold;">ä¼˜åŠ¿ç­–ç•¥</span>@@
@@html:</span>@@
@@html:</div>@@

@@html:<span class="fragment fade-inâ€œ; style="color:#0000FF; font-weight:bold"; data-fragment-index="2">@@ä¼˜åŠ¿ç­–ç•¥@@html:</span>@@

 *** å¯¹å†³
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in cube-out'
:END:
#+CAPTION[å¯¹å†³]:
#+NAME: å¯¹å†³
#+ATTR_HTML: :alt  :title å¯¹å†³ width 800px  :align right
#+attr_org: :width 600px
#+begin_src dot :file img/_generated/dominated_fight_en.png
digraph G {
    node1l [shape=none, margin=0, fontname="LXGW WenKai Mono", label=<
        <TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0" WIDTH="300" HEIGHT="300">
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None"></TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">1,-1</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">2,0</TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,1</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,0</TD>
            </TR>
        </TABLE>
    >];
}
#+end_src

#+ATTR_REVEAL: :frag (appear)V
#+RESULTS[2449c2a972a9cacdd64c521f3637d10264d15bf1]: å¯¹å†³
[[file:img/_generated/dominated_fight_en.png]]

#+BEGIN_NOTES
- ç©å®¶:æˆ‘æ–¹-æ¨ªå‘-A;å¯¹æ‰‹-çºµå‘-B
- ç­–ç•¥:fight,quit
- æ”¶ç›Š:å·¦ä¾§æˆ‘æ–¹,å³ä¾§å¯¹æ‰‹
- ä»£ä»·:-1,å¥–åŠ±2
#+END_NOTES

 *** å¯¹å†³
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none'
:END:


#+CAPTION[å¯¹å†³0]:
#+NAME: å¯¹å†³0
#+ATTR_HTML: :alt  :title å¯¹å†³0 width 800px  :align right
#+attr_org: :width 600px
#+begin_src dot :file img/_generated/dominated_fight0_en.png
digraph G {
    node1 [shape=none, margin=0, fontname="LXGW WenKai Mono", label=<
        <TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0" WIDTH="300" HEIGHT="300">
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None"></TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">1,-1</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">2,<u><FONT COLOR="red">0</FONT></u></TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,<u><FONT COLOR="red">1</FONT></u></TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,0</TD>
            </TR>
        </TABLE>
    >];
}
#+end_src

#+RESULTS: å¯¹å†³0
[[file:img/_generated/dominated_fight0.png]]


 *** å¯¹å†³
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none'
:END:

#+CAPTION[å¯¹å†³1]:
#+NAME: å¯¹å†³1
#+ATTR_HTML: :alt  :title å¯¹å†³1 width 800px  :align right
#+attr_org: :width 600px
#+begin_src dot :file img/_generated/dominated_fight1_en.png
digraph G {
    node1 [shape=none, margin=0, fontname="LXGW WenKai Mono", label=<
        <TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0" WIDTH="300" HEIGHT="300">
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None"></TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1"><u><FONT COLOR="darkgreen">1</FONT></u>,-1</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1"><u><FONT COLOR="darkgreen">2</FONT></u>,<u><FONT COLOR="red">0</FONT></u></TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,<u><FONT COLOR="red">1</FONT></u></TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,0</TD>
            </TR>
        </TABLE>
    >];
}
#+end_src

#+RESULTS: å¯¹å†³1
[[file:img/_generated/dominated_fight1_en.png]]

 *** ä¼˜åŠ¿ç­–ç•¥
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none'
:END:

#+CAPTION[å¯¹å†³2]:
#+NAME: å¯¹å†³2
#+ATTR_HTML: :alt  :title å¯¹å†³2 width 800px  :align right
#+attr_org: :width 600px
#+begin_src dot :file img/_generated/dominated_fight2_en.png :cache yes
digraph G {
    node1 [shape=none, margin=0, fontname="LXGW WenKai Mono", label=<
        <TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0" WIDTH="300" HEIGHT="300">
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None"></TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None"><u><font color="darkgreen">fight</font></u></TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="lightgreen" BORDER="1"><u><FONT COLOR="darkgreen">1</FONT></u>,-1</TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="lightgreen" BORDER="1"><u><FONT COLOR="darkgreen">2</FONT></u>,<u><FONT COLOR="red">0</FONT></u></TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,<u><FONT COLOR="red">1</FONT></u></TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,0</TD>
            </TR>
        </TABLE>
    >];
}
#+end_src
#+RESULTS[addb2dfd97f146f2402e5e559581020c7d5b3d18]: å¯¹å†³2
[[file:img/_generated/dominated_fight2_en.png]]

#+end_comment

** 1.2 Evenly Matched
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in cube-out'
:END:

@@html:<div class="r-stack">@@
        @@html:<img class="fragment fade-out data-fragment-index="0" src="img/drl101/ne_en.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="0" src="img/drl101/ne1_en.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="1" src="img/drl101/ne2_en.png" />@@
        @@html:<img class="fragment" data-fragment-index="2" src="img/drl101/ne3_en.png" />@@
        @@html:<img class="fragment" data-fragment-index="4" src="img/drl101/mixed1_en.png" style="height:400px" />@@
@@html:</div>@@
@@html:<span class="fragment"; style="color:darkgreen; font-weight:bold"; data-fragment-index="2">@@Strategy Equilibrium State@@html:</span>@@
@@html:<div class="fragment"; style="color:darkred; font-weight:bold"; data-fragment-index="3">@@?@@html:</div>@@

#+BEGIN_NOTES
- No dominant strategy.
  - When certain problems have no answer, approaching them from another angle or level can reveal more intriguing phenomena or more important issues.
- The overlap of strategies between rows and columns (two opponents) is the more crucial question.
  - If one side chooses to attack while the other side retreats, neither side wants to deviate from this stateâ€”this is the equilibrium (Nash equilibrium).
#+END_NOTES

#+BEGIN_NOTES
- The premise is that both sides decide simultaneously and do not know each otherâ€™s strategy!
- Strategy equilibrium is bounded by a stable, balanced state.
- A strategy equilibrium is most reasonable and optimal for both sides: if any party deviates from the equilibrium and the other side keeps rational decisions, the deviating sideâ€™s payoff suffers, so no one wants to deviate from the optimal decision within this equilibrium.
- Aâ€™s irrational decision vs. Bâ€™s rational decision.
- Rational decisions are better than irrational ones.
- How many equilibrium states are there?
#+END_NOTES

#+begin_comment

 *** åŠ¿å‡åŠ›æ•Œ
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none'
:END:
#+CAPTION[æŠ€æœ¯è¿›æ­¥]: åŠ¿å‡åŠ›æ•Œ
#+NAME: åŠ¿å‡åŠ›æ•Œ
#+ATTR_HTML: :alt  :title åŠ¿å‡åŠ›æ•Œ width 800px  :align right
#+attr_org: :width 600px
#+begin_src dot :file img/_generated/ne_en.png :cache yes
digraph G {
    node1 [shape=none, margin=0, fontname="LXGW WenKai Mono", label=<
        <TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0" WIDTH="300" HEIGHT="300">
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None"></TD>:w;
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">-1,-1</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">2,0</TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,2</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,0</TD>
            </TR>
        </TABLE>
    >];
}
#+end_src

#+RESULTS[7b778aa5c69ed2026c2fd7acecfcb3a9d1bc5063]: åŠ¿å‡åŠ›æ•Œ
[[file:img/_generated/ne_en.png]]

 *** åŠ¿å‡åŠ›æ•Œ
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none'
:END:

#+CAPTION[æŠ€æœ¯è¿›æ­¥]: åŠ¿å‡åŠ›æ•Œ1
#+NAME: åŠ¿å‡åŠ›æ•Œ1
#+ATTR_HTML: :alt  :title åŠ¿å‡åŠ›æ•Œ1 width 800px  :align right
#+attr_org: :width 600px
#+begin_src dot :file img/_generated/ne1_en.png :cache yes
digraph G {
    node1 [shape=none, margin=0, fontname="LXGW WenKai Mono", label=<
        <TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0" WIDTH="300" HEIGHT="300">
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None"></TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">-1,-1</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">2,<u><FONT COLOR="red">0</FONT></u></TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,<u><FONT COLOR="red">2</FONT></u></TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,0</TD>
            </TR>
        </TABLE>
    >];
}
#+end_src

#+RESULTS[7626c04c7f9bd1a2a8499bc9b74bcdf6b33b866a]: åŠ¿å‡åŠ›æ•Œ1
[[file:img/_generated/ne1_en.png]]

 *** åŠ¿å‡åŠ›æ•Œ
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none'
:END:

#+CAPTION[æŠ€æœ¯è¿›æ­¥]: åŠ¿å‡åŠ›æ•Œ2
#+NAME: åŠ¿å‡åŠ›æ•Œ2
#+ATTR_HTML: :alt  :title åŠ¿å‡åŠ›æ•Œ2 width 800px  :align right
#+attr_org: :width 600px
#+begin_src dot :file img/_generated/ne2_en.png :cache yes
digraph G {
    node1 [shape=none, margin=0, fontname="LXGW WenKai Mono", label=<
        <TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0" WIDTH="300" HEIGHT="300">
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None"></TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">-1,-1</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1"><u><FONT COLOR="darkgreen">2</FONT></u>,<u><FONT COLOR="red">0</FONT></u></TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1"><u><FONT COLOR="darkgreen">0</FONT></u>,<u><FONT COLOR="red">2</FONT></u></TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,0</TD>
            </TR>
        </TABLE>
    >];
}
#+end_src

#+RESULTS[f9aa49ca8e5f6d979f3912aa1940b9545020661e]: åŠ¿å‡åŠ›æ•Œ2
[[file:img/_generated/ne2_en.png]]





 *** ç­–ç•¥å‡è¡¡
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none-in cube-out'
:END:

#+CAPTION[çº³ä»€å‡è¡¡]: çº³ä»€å‡è¡¡
#+NAME: åŠ¿å‡åŠ›æ•Œ3
#+ATTR_HTML: :alt  :title åŠ¿å‡åŠ›æ•Œ3 width 800px  :align right
#+attr_org: :width 600px
#+begin_src dot :file img/_generated/ne3_en.png :cache yes
digraph G {
    node1 [shape=none, margin=0, fontname="LXGW WenKai Mono", label=<
        <TABLE BORDER="0" CELLBORDER="0" CELLSPACING="0" CELLPADDING="0" WIDTH="300" HEIGHT="300">
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None"></TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">fight</TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">-1,-1</TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="lightgreen" BORDER="1"><u><FONT COLOR="darkgreen">2</FONT></u>,<u><FONT COLOR="red">0</FONT></u></TD>
            </TR>
            <TR>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="None">quit</TD>
                <TD WIDTH="100" HEIGHT="100" BGCOLOR="lightgreen" BORDER="1"><u><FONT COLOR="darkgreen">0</FONT></u>,<u><FONT COLOR="red">2</FONT></u></TD>
                <TD WIDTH="100" HEIGHT="100" BORDER="1">0,0</TD>
            </TR>
        </TABLE>
    >];
}
#+end_src

#+RESULTS[287fa2d2d9ce888e387cae9c6914f0a138e0ede9]: åŠ¿å‡åŠ›æ•Œ3
[[file:img/_generated/ne3_en.png]]

  #+BEGIN_NOTES
   - å‰ææ¡ä»¶æ˜¯åŒæ—¶å†³ç­–,ä¸çŸ¥é“å¯¹æ–¹çš„ç­–ç•¥!
   - ç­–ç•¥å‡è¡¡é™äºç¨³å®šçš„å¹³è¡¡çŠ¶æ€
   - ç­–ç•¥å‡è¡¡æ˜¯å¯¹åŒæ–¹æœ€åˆç†çš„æœ€ä¼˜çŠ¶æ€ï¼šä»»ä½•ä¸€æ–¹åç¦»å‡è¡¡çŠ¶æ€ï¼Œè€Œå¦ä¸€æ–¹ä¿æŒç†æ€§å†³ç­–ï¼Œéƒ½ä¼šå¯¼è‡´åç¦»æ–¹æ”¶ç›Šå—æŸï¼Œæ‰€ä»¥æ²¡æœ‰ä»»ä½•ä¸€æ–¹æ„¿æ„åç¦»å‡è¡¡çŠ¶æ€ä¸‹çš„æœ€ä¼˜å†³ç­–
   - A éç†æ€§å†³ç­– vs B ç†æ€§å†³ç­–
   - ç†æ€§å†³ç­–ä¼˜äºéç†æ€§å†³ç­–
  #+END_NOTES

#+end_comment



** 1.3 Mixed Strategy
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in none-out'
:END:
#+NAME: Mixed Strategy
#+ATTR_HTML: :alt  :title Mixed Strategy :width 300px  :align center
#+attr_org: :width 300px :align left
[[./img/drl101/mixed1_en.png]]

#+ATTR_REVEAL: :frag (appear)
- Opponentâ€™s Attack Payoff: $\color{red}{PO^{f}=(-1)\times p^{A} + (2)\times (1-p^{A})}$
- Opponentâ€™s Retreat Payoff: $\color{blue}{PO^{q}=(0)\times p^{A} + (0)\times (1-p^{A})}$
- $p^{A}=0.5$ ?
- When is $p^A$ optimal?
  #+ATTR_REVEAL: :frag (appear)
  ğŸ‘‰ Force the opponent into no choice, valid for $\forall\hspace{0.5em}p^{B}$

  #+BEGIN_NOTES
  - Our payoff is -0.5 : 1
  - Opponentâ€™s payoff is 0.5 : 0
  - Our payoff depends on the opponentâ€™s decision!
  - Regardless of the opponentâ€™s strategy, the payoff is the same.
  #+END_NOTES

*** Mixed Strategy
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none'
:END:

#+ATTR_HTML: :alt  :title Mixed Strategy :width 300px  :align center
#+attr_org: :width 300px :align left
[[./img/drl101/mixed1_en.png]]

#+ATTR_REVEAL: :frag (appear)
- Our strategy: $\color{red}{PO^{f}}=\color{blue}{PO^{q}}$ ğŸ‘‰ $p^{A}=\frac{2}{1+2}=\frac{2}{3}$
  #+ATTR_REVEAL: :frag (appear)
  - Payoff?
  - $p^{A}=1$?
- Rational decisions are better than irrational decisions
  #+BEGIN_NOTES
  - Equilibrium strategy: our payoff is $-\frac{2}{3}\times p^{B} + \frac{4}{3}\times (1-p^{B})$
  - Aâ€™s irrational decision (p=1,0.5) vs. Bâ€™s rational decision (p=2/3)
  #+END_NOTES
- The equilibrium of a mixed strategy most reasonable and optimal
  #+BEGIN_NOTES
   - In a mixed-strategy equilibrium, if either side deviates while the other side remains rational, the deviating sideâ€™s payoff decreases. Hence no one wants to deviate from the equilibriumâ€™s optimal decision.
  #+END_NOTES
- A random strategy is superior to a deterministic one
  #+BEGIN_NOTES
   - Randomness is an efficient model for dealing with complex phenomena.
   - How do we choose an optimal strategy from random strategies? We compute the probability distribution of signals to find the optimal strategy matching our goals.
  #+END_NOTES

*** Mixed Strategy
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none-in cube-out'
:END:

#+ATTR_HTML: :alt  :title Mixed Strategy width 300px  :align center
#+attr_org: :width 400px :align left
[[./img/drl101/mixed1_en.png]]

- Ongoing repeated showdown?

** 1.4 Over the River of Time

*** Decision Tree
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in none-out'
:END:
#+REVEAL_HTML: <div class="gridded_frame_with_columns">
     #+REVEAL_HTML: <div class="one_of_2_columns">
        @@html:<div class="r-stack">@@
         @@html:<img class="fragment fade-out data-fragment-index="0" src="img/drl101/mixed1_en.png" />@@
         @@html:<img class="fragment current-visible" data-fragment-index="0" src="img/drl101/flat_tree.png" />@@
         @@html:<img class="fragment" data-fragment-index="1" src="img/drl101/flat_tree2.png" />@@
        @@html:</div>@@
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_2_columns">
        @@html:<div class="r-stack">@@
         @@html:<img class="fragment fade-out data-fragment-index="2" src="img/drl101/tree.png" />@@
         @@html:<img class="fragment" data-fragment-index="2" src="img/drl101/flat_tree3.png" />@@
        @@html:</div>@@
     #+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

#+BEGIN_NOTES
- Decision trees are almost the only model for decision theory (reinforcement learning).
- Crucial for understanding time-series.
#+END_NOTES

** 1.5 Continuous Showdown
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none-in cube-out'
:END:

@@html:<div class="r-stack">@@
        @@html:<img class="fragment fade-out data-fragment-index="0" src="img/drl101/tree21.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="0" src="img/drl101/tree3.png" />@@
        @@html:<img class="fragment" data-fragment-index="1" src="img/drl101/tree4.png" />@@
@@html:</div>@@

#+ATTR_REVEAL: :frag (appear)
- Work backward step by step: analyze starting from the last round
- Probability of attack $\mathcal{P}=\frac{v}{v+c}: \frac{2}{3}\searrow 0,\ \textrm{if}\ v:2\searrow 0$
- Value function: the long-term value of the current decision and state

#+BEGIN_NOTES
- The complexity of time series increases exponentially!
- Biological evolution is also tree-structured.
- The branching evolution of cause-and-effect sequences.
#+END_NOTES

#+BEGIN_NOTES
- Optimal decisions must account for the long-term consequences of short-term actions.
  - There is a fundamental principle (akin to basic physical laws) allowing quick assessment of the long-term consequences of short-term actions:
    - Natural world, accumulated experience.
    - Optimal control, reinforcement learning.
- How do we evaluate these consequences?
  - Approach it like accumulated experience, building up a â€œvalue function.â€
#+END_NOTES


* Optimal Control, Reinforcement Learning, and Robotics
** 2.1 Review
*** Ten Years Ago

#+REVEAL_HTML: <div class="gridded_frame_with_columns">
     #+REVEAL_HTML: <div class="one_of_2_columns">
        #+attr_org: :width 300px :align left
        #+REVEAL_HTML: <iframe title="PR2" width="600" height="450" src="https://www.youtube.com/embed/gYqfa-YtvW4" frameborder="0" allow="fullscreen; autoplay" allowfullscreen muted></iframe>
        #+REVEAL_HTML: <figcaption>PR2</figcatption>
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_2_columns">
        #+attr_org: :width 300px :align left
        #+REVEAL_HTML: <iframe title="ASIMO" width="600" height="450" src="https://www.youtube.com/embed/xjXUyLAHR1E" frameborder="0" allow="fullscreen; autoplay" allowfullscreen muted></iframe>
        #+REVEAL_HTML: <figcaption>ASIMO</figcatption>
     #+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

#+BEGIN_NOTES
- PR2
  - In 2010, Willow Garage (ROS, Andrew Ng)
  - Actuators, sensors (depth camera, lidar), body, joints
  - Costs are going down
  - Body is more bionic (more complex)
- Asimo
  - Difference between gait control and modern robots
  - Current reference: https://www.youtube.com/watch?v=6CjxMPg0pvg
#+END_NOTES

*** Optimal Control

#+REVEAL_HTML: <iframe width="1024" height="576" src="https://www.youtube.com/embed/OmpzeWym7HQ#t=12m45s" frameborder="0" allow="fullscreen; autoplay" allowfullscreen muted></iframe>
#+REVEAL_HTML: <figcaption>John Tsitsiklis (OG)</figcatption>
#+BEGIN_NOTES
2019 talk
- Early control theory: PID feedback control, linear control; assumes linear systems, simple and elegant
  - Unspecific application to other control targets led to complex expert systems and complicated engineering projects
  - Feedback control theory: zero-pole compensation â†’ cancels the systemâ€™s original dynamics, not using the systemâ€™s own dynamics
- 1990s, 12:45: Optimal control (approximate dynamic programming) vs. reinforcement learning
  - The main difference is that RL emphasizes interaction with the environment, learning-based
  - Optimal control (approximate dynamic programming, adaptive control, robust control) focuses on system identification and model approximations
  - Shallow neural networks, no deep learning
  - Dimitri P. Bertsekas
- 14m18s~15m57s;
- Will briefly comment on AlphaGoâ€™s algorithm
#+END_NOTES

*** Reinforcement Learning
#+REVEAL_HTML: <div class="gridded_frame_with_columns">
     #+REVEAL_HTML: <div class="one_of_2_columns">
        #+ATTR_HTML: :alt  :title Year_Of_RL width 400px  :align center
        #+attr_org: :width 300px :align left
        [[./img/drl101/jim_fan.png]]
        #+REVEAL_HTML: <figcaption>2025: The Year of Reinforcement Learning</figcatption>
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_2_columns">
        #+ATTR_HTML: :alt  :title R1 width 400px  :align center
        #+attr_org: :width 300px :align left
        [[./img/drl101/deepseek_r1_arxiv.png]]
        #+REVEAL_HTML: <figcaption>DeepSeek R1</figcatption>
     #+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

** 2.2 Model
#+ATTR_HTML: :border 2 :class noboldheader
| Game/Conflict                                       | @@html:Players@@                                          | Payoff @@html:<br>@@(Cost)                           | Strategy                                          | State                                            | Strategy Evaluation                                                      |
|-----------------------------------------------------|-----------------------------------------------------------|------------------------------------------------------|----------------------------------------------------|--------------------------------------------------|-------------------------------------------------------------------------|
| {{{a(<font color=darkblue>Reinforcement Learning</font>,1)}}} | {{{a(<font color=darkblue>Agent/<br>System</font>,1)}}} | {{{a(<font color=darkblue>Reward</font>,1)}}} | {{{a(<font color=darkblue>Action</font>,1)}}}     | {{{a(<font color=darkblue>State</font>,1)}}}     | {{{a(<font color=darkblue>Value Function</font>,1)}}}          |
| {{{a(<font color=red>Optimal Control</font>,2)}}}             | {{{a(<font color=red>Controller/<br>Object</font>,2)}}}  | {{{a(<font color=red>Error</font>,2)}}}    | {{{a(<font color=red>Control Variable</font>,2)}}} | {{{a(<font color=red>State</font>,2)}}}          | {{{a(<font color=red>Objective Function</font>,2)}}}                  |

#+BEGIN_NOTES
  - Players: humans vs. humans, computers, nature/physical laws;
  - Computer vs. nature/physical laws
  - Adding observations and value estimates
    - In reinforcement learning, the value function is learned and depends on the system state and actions, taking into account system equations and dynamics
    - In optimal control, the objective function is based on expert rules and doesnâ€™t consider system dynamics!
#+END_NOTES

#+ATTR_REVEAL: :frag (appear)
#+attr_html: :alt :title Reinforcement Learning Model :width 750pix :align center
#+NAME: Reinforcement Learning Model
#+attr_org: :width 300px :align left
[[./img/drl101/rl_model_en.png]]

** 2.3 Methods in Reinforcement Learning
#+ATTR_REVEAL: :frag (appear)
#+begin_quote
Solve complex problems step by step
#+end_quote
  #+ATTR_REVEAL: :frag (appear)
  - "Given the present, the future is independent of the past"
    #+ATTR_REVEAL: :frag (appear)
    ğŸ‘‰ Markov Decision Process
  - Complex problems can be decomposed into subproblems
    #+ATTR_REVEAL: :frag (appear)
    ğŸ‘‰ Dynamic Programming
  - Estimate the values of states and actions from fragmented experiences
    #+ATTR_REVEAL: :frag (appear)
    ğŸ‘‰ Bellman Equation
#+BEGIN_NOTES
  - Understanding the concept is more important than memorizing its name
  - Dynamic programming is the mainstream classical concept and the foundation of optimal control
  - The shortest path from A to B can be divided into two stages, A to C and C to B: if the path from C to B is shortest, then you only need to solve the subproblem from A to C!
#+END_NOTES

*** Rational Decision Making
#+ATTR_REVEAL: :frag (appear)
#+begin_quote
- Algorithms embody rational decision making
- Rational decisions serve as a counterstrategy against irrational decisions
#+end_quote

#+BEGIN_NOTES
  - AlphaGo is very hard to beat; humans are hard to beat by machines: perfect memory, pure rationality, efficient execution, and replicable behavior
  - There is no predetermined purpose
  - Jeff Hintonâ€™s warning
  - Originally, reinforcement learning was a relatively obscure area in AI; its biggest difference from optimal control is the incorporation of learning.
    - Why has it become mainstream in AI and robotics since 2016? --> Deep Learning.
    - How to combine the two? Sampling! Learning from fragmented experiences.
#+END_NOTES

** 2.4 Learning from Fragmented Experiences

*** Random Sampling
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in cube-out'
:END:

@@html:<div class="r-stack">@@
        @@html:<img class="fragment fade-out" data-fragment-index="0" src="img/drl101/tree_sample0.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="0" src="img/drl101/tree_sample.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="1" src="img/drl101/tree_sample1.png" />@@
        @@html:<img class="fragment current-visible" data-fragment-index="2" src="img/drl101/tree_sample2.png" />@@
        @@html:<img class="fragment" data-fragment-index="3" src="img/drl101/tree_sample3.png" />@@
@@html:</div>@@

*** Advantages of Random Sampling

#+attr_html: :alt :title Random Decision Sampling :width 500pix :align center
#+NAME: Random Decision Sampling
#+attr_org: :width 300px
[[./img/drl101/tree_sample1.png]]

#+ATTR_REVEAL: :frag (appear)
- Real data
  #+ATTR_REVEAL: :frag (appear)
  - Modeling complexity is too high
- Complex functions/distributions:
  #+ATTR_REVEAL: :frag (appear)
  - Nonlinear
  - Time-varying and non-stationary processes
- Natural laws
- An efficient way to tackle complex problems
- Allows learning from fragmented experiences
#+BEGIN_NOTES
- Rolling dice is often the most efficient learning method in complex, random environments
- Deterministic mathematical problems can often be solved in an elegant and efficient way using probabilistic methods (combinatorics)
#+END_NOTES

*** Optimal Control vs. Reinforcement Learning

#+REVEAL_HTML: <div class="gridded_frame_with_columns">
     #+REVEAL_HTML: <div class="one_of_2_columns">
     @@html:<div class="r-stack">@@
        @@html:<img class="fragment" data-fragment-index="0" src="img/drl101/hiking.jpg" height="400px"/>@@
     @@html:</div>@@
     @@html:<div class="r-stack">@@
        @@html:<div class="centered"><span class="fragment" data-fragment-index="0">@@Optimal Control@@html:</span></div>@@
     @@html:</div>@@
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_2_columns">
     @@html:<div class="r-stack">@@
        @@html:<img class="fragment current-visible" data-fragment-index="1" src="img/drl101/surfing.jpg" height="400px"/>@@
        @@html:<img class="fragment" data-fragment-index="2" src="img/drl101/skateboarding.jpg" height="400px"/>@@
     @@html:</div>@@
     @@html:<div class="r-stack">@@
        @@html:<div class="centered"><span class="fragment current-visible" data-fragment-index="1">@@Reinforcement Learning@@html:</span></div>@@
        @@html:<div class="centered"><span class="fragment" data-fragment-index="2">@@Robot Reinforcement Learning@@html:</span></div>@@
     @@html:</div>@@
     #+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

** 2.5 Model Complexity

*** AlphaGoâ€™s State and Decision Tree
#+attr_html: :alt :title AlphaGo Decision Tree :width 800pix :align center
#+NAME: AlphaGo Decision Tree
#+attr_org: :width 300px
[[./img/drl101/MCTS-in-AlphaGo.png]]

#+ATTR_REVEAL: :frag (appear)
- Value: can be interpreted as the win rate
*** AlphaGoâ€™s State and Decision Tree
#+attr_html: :alt :title AlphaGo Decision Tree :width 800pix :align center
#+NAME: AlphaGo Decision Tree
#+attr_org: :width 300px
[[./img/drl101/alphago_mcts.png]]

*** AlphaGoâ€™s Complexity

#+ATTR_REVEAL: :frag (appear)
- All positions (observations): $3^{{19}^2}\approx 1.74\times 10^{172}$, with $1.20\%$ legal moves
- Approximately ~200 moves per game, with an average of ~$3\times 10^{511}$ different games
- Theoretically, the maximum number of moves is $10^{48}$, with the number of different games ranging from $10^{10^{48}}$ to $10^{10^{171}}$
- Number of atoms in the observable universe: $10^{80}$
  #+ATTR_REVEAL: :frag (appear)
  ğŸ‘‰  Neural Networks
#+BEGIN_NOTES
 - Number of atoms: Eddington number
 - Learning from complete but partial experience: learning from fragments of games and accumulating experience
 - Learning from incomplete experience: online learning, where learning happens concurrently with gameplay
 - It has been solvedâ€”a truly awe-inspiring achievement!
   - A summary of human wisdom and experience: offense and defense, formations, life-and-death, endgame, coordination, overall situation, techniques, and tactics
   - Efforts to summarize human experience using feature methods cannot compete with AlphaGo
 - The bitter lessons of artificial intelligence
#+END_NOTES

*** The State and Complexity of a Bipedal Robot

#+REVEAL_HTML: <div class="gridded_frame_with_columns">
     #+REVEAL_HTML: <div class="one_of_2_columns">
        #+REVEAL_HTML: <iframe width="600" height="450" src="https://www.youtube.com/embed/0OUavEtbt2E#t=6m03s" frameborder="0" allow="fullscreen; autoplay" allowfullscreen muted></iframe>
        #+REVEAL_HTML: <figcaption>Cassie Model</figcatption>
        #+BEGIN_NOTES
          - Another example of embodied intelligence: Cassie, a bipedal robot
          - 5m57s~6m27s, 7m08s~8:45s:
            - Inertia/mass matrix is positive definite, exhibiting high complexity
            - The systemâ€™s state and dynamics, and the policy (controller)
            - Objectives (payoff, control trajectory) and strategy evaluation (player)
          - Complexity:
            - Dynamic systems with long-term influence of control variables
            - Partial observability/randomness
            - Nonlinearity
            - Legged robots are underactuated systems,
              - Deliberately so; they are harder to control but more natural and energy-efficient. A natural gait is the optimal control solutionâ€”moving with minimal energy in an economical manner (the way control variables affect state variables)!
        #+END_NOTES
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_2_columns">
        #+ATTR_REVEAL: :frag (appear)
        - Methods for controlling complex objects:
          #+ATTR_REVEAL: :frag (appear)
          - Optimal control
            #+BEGIN_NOTES
            - Lagrangian mechanics: force/torque â†’ action; the time integral of energy (kinetic and potential) changes; force/energy variations produce motion
            - Principle of steady-state action (motion follows energy equilibrium, conservation): every systemâ€™s dynamics follows a unique path
            - Since the 1990s, two approaches have emerged
            - There is no simple, magical method for handling complex phenomenaâ€”it requires significant computational resources and the key is how to apply them: either for system identification or for stepwise digestion of fragmented experience data
            - Optimal control, via approximate dynamic programming (Approximate DP): it uses precise environmental and dynamic models to capture the primary contradictions; however, its drawbacks include specificity to the model, being tailored for particular scenarios (difficult to generalize), and poor robustness against disturbances
            #+END_NOTES
          #+ATTR_REVEAL: :frag (appear)
          - Reinforcement learning
            #+BEGIN_NOTES
            - Uses random and probabilistic models to learn in a manner similar to how nature and humans solve problems
            - The systemâ€™s state and policy are obtained through learning
            - Why is reinforcement learning effective in handling complex problems?
            #+END_NOTES
        - How to learn?
          #+BEGIN_NOTES
          - Primarily due to breakthroughs in deep learning
          - In practice, complex system dynamics are learned from fragmented experiences
          - Evaluation of complex value functions and policies is achieved!
          #+END_NOTES
     #+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

** 2.6 Machine Learning for Robotics
#+ATTR_REVEAL: :frag (appear)
- Each demonstration is a path on the decision tree
- Data density obtained through random sampling
- Experiences of success and failure
#+BEGIN_NOTES
  - Reinforcement learning training
  - Covering observation data distributions specific to functionalities
  - Paths representing success or failure
#+END_NOTES

*** The Role of Simulation
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in cube-cout'
:END:

#+REVEAL_HTML: <div class="gridded_frame_with_columns">
     #+REVEAL_HTML: <div class="one_of_3_columns">
        #+ATTR_HTML: :alt  :title  :width 400pix  :align center
        #+attr_org: :width 400px :align left
        #+CAPTION: Grasp
        #+NAME: pick
        [[https://developer-blogs.nvidia.com/wp-content/uploads/2022/07/image16.gif]]
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_3_columns">
        #+ATTR_HTML: :alt  :title tree :width 400pix  :align center
        #+attr_org: :width 400px :align left
        #+CAPTION: Position
        #+NAME: position
        [[https://developer-blogs.nvidia.com/wp-content/uploads/2022/07/image5-1.gif]]
     #+REVEAL_HTML: </div>
     #+REVEAL_HTML: <div class="one_of_3_columns">
        #+ATTR_HTML: :alt  :title tree :width 400pix  :align center
        #+attr_org: :width 400px :align left
        #+CAPTION: Operation
        #+NAME: operation
        [[https://developer-blogs.nvidia.com/wp-content/uploads/2022/07/image6.gif]]
     #+REVEAL_HTML: </div>
#+REVEAL_HTML: </div>

*** Simulation Data for Training
#+CAPTION[robot learning]: training dataset generation
#+REVEAL_HTML: <iframe width="640" height="360" src="https://www.youtube.com/embed/OAZrBYCLnaA" frameborder="0" allow="fullscreen; autoplay" allowfullscreen muted></iframe>
#+REVEAL_HTML: <figcaption>Nvidia Isaac Sim</figcatption>
#+BEGIN_NOTES
  - 15:24 ~ 16:57
  - ACRONYM: Nvidia FLEX
  - Imagine describing it in a modeling framework
  - Skill requirements for application engineers: may not necessarily include programming
#+END_NOTES

*** Algorithms for Robot Learning

#+ATTR_REVEAL: :frag (appear)
- Data
  #+ATTR_REVEAL: :frag (appear)
  - Sources: online/offline/(simulation)
  - Pre-training (foundation models such as GPT)
  - Data paradigms (training planning/data/diversity construction)
- Learning Models
  #+ATTR_REVEAL: :frag (appear)
  - Robustness
  - Diversity
#+BEGIN_NOTES
  - Learning models: representational learning, neural networks
  - Data is extremely important
    - Online/offline
    - Pre-training (foundation models like GPT)
      - The foundation model provides common sense and fundamental reasoning ability; cross-domain learning (e.g., autonomous driving experience can benefit humanoid robotics performance)
    - Data diversity is crucial; multimodal robot data is more significant: training the same model can improve average performance by over 50%
  - Efficient learning models can capture complex behavioral patterns (multimodal)
#+END_NOTES

** 2.7 Understanding AlphaGo

*** AlphaGo System Architecture
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in none-out'
:END:

#+attr_html: :alt :title AlphaGo Neural Network :width 600pix :align center
#+NAME: AlphaGo Neural Network
#+attr_org: :width 300px
[[./img/drl101/alphago_nn.png]]

#+ATTR_REVEAL: :frag (appear)
- Learning from fragmented experiences
  #+ATTR_REVEAL: :frag (appear)
  - Accumulation of partial experiences
    - ğŸ‘‰ Neural Networks
  - Aggregation of incomplete experiences
    - ğŸ‘‰ Online Learning
- Randomness and probability are effective models for addressing complexity
  #+ATTR_REVEAL: :frag (appear)
  - Value network: a simple win-rate lookup table
#+BEGIN_NOTES
- One can complete a game and then learn from it
- It is possible to learn while playing (temporal difference learning)
- Decision network,
- Although the complexity of Go is extremely high, it is a deterministic game
#+END_NOTES

*** Optimal Strategy
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'none-in cube-out'
:END:
#+attr_html: :alt :title AlphaGo Neural Network :width 800pix :align center
#+NAME: AlphaGo Neural Network
#+attr_org: :width 300px
[[./img/drl101/alphago_nn.png]]

#+ATTR_REVEAL: :frag (appear)
- Equilibrium Strategy
  #+ATTR_REVEAL: :frag (appear)
  - The equilibrium of mixed strategies represents the most optimal and reasonable state for both sides
  - Rational decision making outperforms irrational decision making
- Self-training / Self-learning
  #+ATTR_REVEAL: :frag (appear)
  - Equilibrium State (Optimal Strategy)

#+BEGIN_NOTES
 - Why is the equilibrium strategy optimal?
   - In a mixed-strategy equilibrium, if one side deviates while the other maintains rationality, the deviating sideâ€™s payoff will decreaseâ€”thus, no side has an incentive to deviate from the optimal decision under equilibrium.
   - Intuition: Establishing an invincible defense before attacking the opponent
 - Aâ€™s irrational decision (human players) vs. Bâ€™s rational decision (AlphaGo)
 - Nash Equilibrium: In self-play, my decision must render the opponentâ€™s payoff identical regardless of their choice
   - Self-training / self-learning: a dual-combat process
   - Why self-training leads to improvement:
     - Mathematically: under reasonable assumptions (payoff > cost, v > c), a rational decision implies that at equilibrium, the first derivative of the payoff is zero and the second derivative is negative.
#+END_NOTES

* Summary
:PROPERTIES:
:REVEAL_DATA_TRANSITION: 'cube-in none-out'
:END:

#+ATTR_REVEAL: :frag (fade-in) :frag_idx (1 2 3)
- Optimal Strategy
  - Optimal decision making must consider the opponent's decisions
- Machine Learning
  - @@html:<span class="r-stack">@@
    @@html:<span class="fragment fade-out"; data-fragment-index="4">@@Random sampling is an efficient method for tackling complex problems@@html:</span>@@
    @@html:<span class="fragment fade-in"; style="color:#FF0000; font-weight:bold"; data-fragment-index="4">@@Random sampling is an efficient method for tackling complex problems@@html:</span>@@
    @@html:</span>@@
- Neural Networks
  - Randomness and probability are effective models for addressing complex phenomena
