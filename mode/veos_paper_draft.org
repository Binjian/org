:PROPERTIES:
:ID:       f949414e-7ddf-4d0f-b2b0-d27c2644a498
:END:
#+title: veos-paper draft

* Introduction
** motivation
*** theoretical significance
alex irpan paper post why reinforcement leanring does not work yet in the real world
scaling up model, is not the case
1. Simulation atari
2. Carla
3. server cluster VAC
4. Bonsai

make some progress, any progress in the application;

*** energy consumption optimization:

while meeting the desired vehicle control (driving dynamics). The dynamics will be changed in a way that is energy efficient. In how far the dynamics will be changes, while guaranteeing the motion planning and safety.

*** Industrial significance:
1. saving energy,
2. extending driving range

#+begin_quote
learning based.

driving decision with short horizon has an impact on the overall energy consumption

no realtime requirement, based on the scenario, minutes would be sufficient. actuallly the luxuray of seconds updating the nearby rows of torque table.

Contextual Information like NLU different scenario, any action is like tokens. --> Sequential decision by transformer/RNN

extract driving mode from large data in a online-learning mode.

#+end_quote

*** A new domain for AI application
**** human in the loop,
**** cooperative learning

** sota
*** physical model based

**** [1]Data-driven Framework for Fuel Efficiency Improvement in Extended Range Electric Vehicle Used in Package Delivery Application
   2020, specific model (physical model, EREV) , state space: observe SoC, f, GPS; action space: range of $L_{set}$  *Energy Compensated expected trip distance*
   change only scalar action value, no high dyanmic

**** [999] RL based Energy Optimization for a Fuel Cell Electric Vehicle, overly complicated model parameters
 state: power demand and soc, ( $P_{dem}$ , SOC) difficult to observe, reward functoin
 $R_{t}(\dot{m}_{fc},P_{b},R_{b},Q_{m},U_{ocv})$ contains instantaneous hydrogen consumption, battery power, battery internal resistance, battery capacity, open-circuit voltage; action/control variable $A=(P_{fc})$ is the fuel cell power
**** [111] Reinforcement learning for electric vehicle applications in power systems: a critical review

* method

** model
*** vehicle dynamic system modeling
**** general model
*** reinforcement learning model
**** action model: torque model, translational mixed gaussian model, with speed translation invariance
equation
**** observation model: state,
equation
**** **reward model**
no artificial reward points but true reward, the energy consumuption
**** driver model
driving style

** time sequence is important for exact reward
** data pool with dask and mongodb for easy sampling, storing, indexing, data interface with DataFrame with every timestamps
** system
*** signal interface
*** signal flow diagram
* experiment results discussion
** ddpg
short period of attention window
** driving style hinted at a common reward of human drive and agent

** rdpg
long episode truncated BPTT long period of attention window
episode management, training selection,

RLHF? easy way with empirical distribution no sequential model, first ignore the time sequence, just to look at the difference.
** driving style
** training schedule is important model for driving style identification e2e way.
*** utilizing offline data CGL
*** improve learning efficiency by
*** federated learning for meta learning,evolving


* MLOps, dataflow and ETL pipelines

\begin{figure}
\begin{center}
    \includegraphics[width=0.86\textwidth, angle=0]{./img/adp_data_flow_seq_block.png}
\end{center}
\caption{Data pipelines}
\end{figure}



* broader impact
NAS,
