:PROPERTIES:
:ID:       f949414e-7ddf-4d0f-b2b0-d27c2644a498
:END:
#+title: veos-paper draft

* Introduction
** motivation
*** theoretical significance
alex irpan paper post why reinforcement leanring does not work yet in the real world
scaling up model, is not the case
1. Simulation atari
2. Carla
3. server cluster VAC
4. Bonsai

make some progress, any progress in the application;



*** Industrial significance:
1. saving energy,
2. extending driving range

#+begin_quote
learning based.

driving decision with short horizon has an impact on the overall energy consumption

no realtime requirement, based on the scenario, minutes would be sufficient. actuallly the luxuray of seconds updating the nearby rows of torque table.

Contextual Information like NLU different scenario, any action is like tokens. --> Sequential decision by transformer/RNN

extract driving mode from large data in a online-learning mode.

#+end_quote



[[edraw:data=H4sIAOPOOWYAA5VR3W6DIBR+FXJ6XRFrl9qIT7A9hFVEIgWDrNi339Fi3MWWZSHwBTjfz4FyekgSVOt7Due3FEgvlOw9hzzDzXzXZuLQez9eKQ0hJOGUWCdplqYpRSqQh3CTsoYDS1gkXGetzPATjRVFQddbqEonGk9mDujzXNdfY6iWg2hdHY63uhmks5+mBTJ5ZwfBwVgjgHRKaw6HruuA0KqU30m2fW5+W9llGbvGgaXL2A6OMUq2B2H5nhC7XzMvQbEBtiB6Cq3VOIlNw451ozyWMfi/rUPeGS8c6l8QG9yflsoGD7JoONa+j9KvR/hbFx/lA+kEW3hfMYuYR4zSVOLED66+AAYcyWYhAgAA]]

** sota
*** physical model based
1. [1]Data-driven Framework for Fuel Efficiency Improvement in Extended Range Electric Vehicle Used in Package Delivery Application
   2020, specific model (physical model, EREV) , state space: observe SoC, f, GPS; action space: range of $L_{set}$  *Energy Compensated expected trip distance*
   change only scalar action value, no high dyanmic
2. [999] RL based Energy Optimization for a Fuel Cell Electric Vehicle
***** state: soc difficult to observer, action/control variable u is $P_{{fc}}$


* method

** model
*** vehicle dynamic system modeling
**** general model
*** reinforcement learning model
**** action model: torque model, translational mixed gaussian model, with speed translation invariance
equation
**** observation model: state,
equation
**** **reward model**
no artificial reward points but true reward, the energy consumuption
**** driver model
driving style

** time sequence is important for exact reward
** data pool with dask and mongodb for easy sampling, storing, indexing, data interface with DataFrame with every timestamps
** system
*** signal interface
*** signal flow diagram
* experiment results discussion
** ddpg
short period of attention window
** driving style hinted at a common reward of human drive and agent

** rdpg
long episode truncated BPTT long period of attention window
episode management, training selection,

RLHF? easy way with empirical distribution no sequential model, first ignore the time sequence, just to look at the difference.
** driving style
** training schedule is important model for driving style identification e2e way.
*** utilizing offline data CGL
*** improve learning efficiency by
*** federated learning for meta learning,evolving

* broader impact
NAS,
