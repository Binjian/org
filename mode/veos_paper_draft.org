:PROPERTIES:
:ID:       f949414e-7ddf-4d0f-b2b0-d27c2644a498
:END:
#+title: veos-paper draft
#+latex: synctex=true
#+bibliography: ../latex/arxiv-style/references.bib
#+bibliography: ../bib/veos.bib
#+bibliography: local.bib

* Introduction
** motivation
*** theoretical significance
alex irpan paper post why reinforcement leanring does not work yet in the real world
scaling up model, is not the case
1. Simulation atari
2. Carla
3. server cluster VAC
4. Bonsai

make some progress, any progress in the application;
understand the driving style with respect to energy consumption

[cite:@Irpan_2018]

Can you really make progress in real world application

advocate a learning based dynamic policy, that is flexible, multimodal in complex real road scenarios

*** energy consumption optimization:

while meeting the desired vehicle control (driving dynamics). The dynamics will be changed in a way that is energy efficient. In how far the dynamics will be changes, while guaranteeing the motion planning and safety.

*** Industrial significance:
1. saving energy,
2. extending driving range

#+begin_quote
learning based.

driving decision with short horizon has an impact on the overall energy consumption

no realtime requirement, based on the scenario, minutes would be sufficient. actuallly the luxuray of seconds updating the nearby rows of torque table.

Contextual Information like NLU different scenario, any action is like tokens. --> Sequential decision by transformer/RNN

extract driving mode from large data in a online-learning mode.

#+end_quote

*** A new domain for AI application
**** human in the loop,
**** cooperative learning
*** BEV driving
**** optimal policy is determined by longitudinal speed control (torque request) and road condition, and driving style
**** real reward,  no artificial point, no human target of reward function, but object reward, same goal if human is neutral or cooperative,
**** short time horizon of several seconds is enough to get a decent policy
**** long time horizon is possible but need more training and scale up.
**** human in the loop, not RLHF, reward is determined by human, subjective,
**** whether the complexity is overparametrized.

** sota
*** physical model based



**** 2020, specific model (physical model, EREV) , state space: observe SoC, f, GPS; action space: range of $L_{set}$  *Energy Compensated expected trip distance*
change only scalar action value, no high dyanmic
[cite:@Wang_2020]
**** [999] RL based Energy Optimization for a Fuel Cell Electric Vehicle, overly complicated model parameters
state: power demand and soc, ( $P_{dem}$ , SOC) difficult to observe, reward functoin
$R_{t[cite:@Hou_2022]}(\dot{m}_{fc},P_{b},R_{b},Q_{m},U_{ocv})$ contains instantaneous hydrogen consumption, battery power, battery internal resistance, battery capacity, open-circuit voltage; action/control variable $A=(P_{fc})$ is the fuel cell power
[cite:@Hou_2022]
**** [111] Reinforcement learning for electric vehicle applications in power systems: a critical review
[cite:@Qiu_2023]

* system
** whole system

#+ATTR_HTML: :alt  :title system architecture :width 600px  :align center
#+attr_org: :width 500px
[[./img/tspace_overview.svg]]

** local interface

#+ATTR_HTML: :alt  :title cloud interface :width 400px :align center
#+attr_org: :width 300px
[[./img/data_flow_kvaser.svg]]

** cloud interface

#+ATTR_HTML: :alt  :title cloud interface :width 600px :align center
#+attr_org: :width 600px
[[./img/data_flow_cloud.svg]]

* method

[[./img/actor-critic.svg]]

[[excalidraw:/Users/x/.org.d/excalidraw/8167a3b6-bf87-44fb-9b85-42f33fcf9b53.excalidraw.svg]]

** model
*** vehicle dynamic system modeling
**** general model
*** reinforcement learning model
**** action model: torque model, translational mixed gaussian model, with speed translation invariance
equation
**** observation model: state,
equation
**** **reward model**
no artificial reward points but true reward, the energy consumuption
**** driver model
driving style
** Inference

[[./img/rl_control.svg]]



** time sequence is important for exact reward
** data pool with dask and mongodb for easy sampling, storing, indexing, data interface with DataFrame with every timestamps
** system
*** signal interface
*** signal flow diagram
** Training mode and Inference mode

* MLOps, dataflow and ETL pipelines

[[./img/adp_data_flow_seq_block.png]]



* experiment results discussion
** ddpg
short period of attention window
** driving style hinted at a common reward of human drive and agent

** rdpg
long episode truncated BPTT long period of attention window
episode management, training selection,

RLHF? easy way with empirical distribution no sequential model, first ignore the time sequence, just to look at the difference.
** driving style
** training schedule is important model for driving style identification e2e way.
*** utilizing offline data CGL
*** improve learning efficiency by
*** federated learning for meta learning,evolving

** controllability
tends to be heavier on the pedal, but

* broader impact
NAS,

* Bibliography
#+print_bibliography:
